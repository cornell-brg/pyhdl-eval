#!/usr/bin/env python
#=========================================================================
# generate [options] prompt-filename
#=========================================================================
#
#  -v --verbose        Display the prompt
#  -h --help           Display this message
#  -l --list-models    List supported models
#     --model          LLM model to use (default: openai/gpt-3.5-turbo)
#     --temperature    LLM model temperature (default: 0.8)
#     --top-p          LLM model top_p (default: 0.95)
#     --max-tokens     LLM model max_tokens (default: 1024)
#     --examples       Number of in-context examples to use (default: 0)
#     --lang           What language to generate (default: verilog)
#     --output         File to write extracted code
#
# Use a LLM to generate verilog or a Python-embedded DSL from the
# given prompt.
#
# SPDX-License-Identifier: MIT
# Author : Christopher Batten, NVIDIA
# Date   : May 20, 2024
#

import argparse
import sys
import os
import re
import time

from langchain_openai import ChatOpenAI
from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain_community.chat_models import ChatAnyscale
from langchain.schema import SystemMessage, HumanMessage
from langchain_community.callbacks import get_openai_callback

#-------------------------------------------------------------------------
# Command line processing
#-------------------------------------------------------------------------

class ArgumentParserWithCustomError(argparse.ArgumentParser):
  def error( self, msg = "" ):
    if ( msg ): print("\n ERROR: %s" % msg)
    print("")
    file = open( sys.argv[0] )
    for ( lineno, line ) in enumerate( file ):
      if ( line[0] != '#' ): sys.exit(msg != "")
      if ( (lineno == 2) or (lineno >= 4) ): print( line[1:].rstrip("\n") )

def parse_cmdline():
  p = ArgumentParserWithCustomError( add_help=False )

  p.add_argument( "-h", "--help",        action="store_true" )
  p.add_argument( "-v", "--verbose",     action="store_true" )
  p.add_argument( "-l", "--list-models", action="store_true" )
  p.add_argument(       "--model",       type=str,   default="openai/gpt3.5-turbo" )
  p.add_argument(       "--temperature", type=float, default=0.8 )
  p.add_argument(       "--max-tokens",  type=int,   default=1024 )
  p.add_argument(       "--top-p",       type=float, default=0.95 )
  p.add_argument(       "--examples",    type=int,   default=0 )
  p.add_argument(       "--lang",        type=str,   default="verilog" )
  p.add_argument(       "--output",      type=str,   default="NONE" )
  p.add_argument( "prompt_filename" )

  opts = p.parse_args()
  if opts.help: p.error()
  return opts

#-------------------------------------------------------------------------
# Models
#-------------------------------------------------------------------------

openai_chat_models = [
  "gpt-3.5-turbo",
  "gpt-4",
  "gpt-4-turbo",
]

nim_chat_models = [
  "ai-gemma-2b",
  "ai-llama2-70b",
  "ai-llama3-70b",
  "ai-codegemma-7b",
  "ai-codellama-70b",
]

anyscale_chat_models = [
  "codellama/CodeLlama-70b-Instruct-hf",
  "google/gemma-7b-it",
  "meta-llama/Meta-Llama-3-8B-Instruct",
  "meta-llama/Meta-Llama-3-70B-Instruct",
  "mistralai/Mistral-7B-Instruct-v0.1",
  "mistralai/Mixtral-8x7B-Instruct-v0.1",
  "mistralai/Mixtral-8x22B-Instruct-v0.1",
]

all_chat_models = [
  *openai_chat_models,
  *nim_chat_models,
  *anyscale_chat_models,
]

model_aliases = {
  "openai/gpt3.5-turbo"    : "gpt-3.5-turbo",
  "openai/gpt4"            : "gpt-4",
  "openai/gpt4-turbo"      : "gpt-4-turbo",

  "nim/gemma-2b"           : "ai-gemma-2b",
  "nim/llama2-70b"         : "ai-llama2-70b",
  "nim/llama3-70b"         : "ai-llama3-70b",
  "nim/codegemma-7b"       : "ai-codegemma-7b",
  "nim/codellama-70b"      : "ai-codellama-70b",

  "anyscale/codellama-70b" : "codellama/CodeLlama-70b-Instruct-hf",
  "anyscale/gemma-7b"      : "google/gemma-7b-it",
  "anyscale/llama3-8b"     : "meta-llama/Meta-Llama-3-8B-Instruct",
  "anyscale/llama3-70b"    : "meta-llama/Meta-Llama-3-70B-Instruct",
  "anyscale/mistral-7b"    : "mistralai/Mistral-7B-Instruct-v0.1",
  "anyscale/mixtral-8x7b"  : "mistralai/Mixtral-8x7B-Instruct-v0.1",
  "anyscale/mixtral-8x22b" : "mistralai/Mixtral-8x22B-Instruct-v0.1",
}

#-------------------------------------------------------------------------
# VerboseOutput
#-------------------------------------------------------------------------

class VerboseOutput:

  def __init__( self, verbose ):
    self.verbose = verbose

  def print( self, string ):
    if self.verbose:
      print( string )

#-------------------------------------------------------------------------
# Context
#-------------------------------------------------------------------------

verilog_system_msg="""
You are a Verilog RTL designer that only writes code using correct
Verilog syntax.
"""

pymtl_system_msg="""
You are a PyMTL3 RTL designer that only writes code using correct
PyMTL3 syntax.
"""

pyrtl_system_msg="""
You are a PyRTL RTL designer that only writes code using correct
PyRTL syntax.
"""

myhdl_system_msg="""
You are a MyHDL RTL designer that only writes code using correct
MyHDL syntax.
"""

migen_system_msg="""
You are a Migen RTL designer that only writes code using correct
Migen syntax.
"""

amaranth_system_msg="""
You are a Amaranth RTL designer that only writes code using correct
Amaranth syntax.
"""

prompt_no_explain_suffix="""
Enclose your code with <CODE> and </CODE>. Only output the code snippet
and do NOT output anything else.
"""

#-------------------------------------------------------------------------
# Main
#-------------------------------------------------------------------------

def main():

  opts = parse_cmdline()

  # Check models

  if opts.list_models:

    print( "" )
    print( "OpenAI Models" )

    for model in openai_chat_models:
      print( f" - {model}" )

    print( "" )
    print( "NIM Models" )

    for model in nim_chat_models:
      print( f" - {model}" )

    print( "" )
    print( "Anyscale Models" )

    for model in anyscale_chat_models:
      print( f" - {model}" )

    print( "" )
    print( "Model Aliases" )

    for key,value in model_aliases.items():
      print( f" - {key} : {value}" )

    print( "" )
    return

  model_alias = opts.model
  model = opts.model
  if opts.model in model_aliases:
    model = model_aliases[opts.model]

  if model not in all_chat_models:
    print("")
    print(f"ERROR: Unknown model {model}")
    print("")
    return

  # Check output language

  system_msg = ""

  if   opts.lang == "verilog":  system_msg = verilog_system_msg
  elif opts.lang == "pymtl":    system_msg = pymtl_system_msg
  elif opts.lang == "pyrtl":    system_msg = pyrtl_system_msg
  elif opts.lang == "myhdl":    system_msg = myhdl_system_msg
  elif opts.lang == "migen":    system_msg = migen_system_msg
  elif opts.lang == "amaranth": system_msg = amaranth_system_msg
  else:
    print("")
    print(f"ERROR: Unknown language {lang}")
    print("")
    return

  # Load the examples

  prompt_example_prefix = ""
  for i in range(opts.examples):
    examples_filename = os.path.dirname(__file__) + f"/../examples/{opts.lang}-ex{i+1}.txt"
    with open(examples_filename) as f:
      prompt_example_prefix += f.read().rstrip() + "\n"

  # Check for an output file

  out = VerboseOutput( opts.verbose )

  # Log parameters

  problem = "?"
  if opts.prompt_filename.endswith("_prompt.txt"):
    problem = os.path.basename(opts.prompt_filename[:-11])

  temperature = opts.temperature
  top_p       = opts.top_p
  max_tokens  = opts.max_tokens

  out.print( "" )
  out.print( f"problem     = {problem}"     )
  out.print( f"model_alias = {model_alias}" )
  out.print( f"model       = {model}"       )
  out.print( f"temperature = {temperature}" )
  out.print( f"top_p       = {top_p}"       )
  out.print( f"max_tokens  = {max_tokens}"  )

  # Read the prompt file

  with open(opts.prompt_filename) as file:
    prompt = file.read()

  # Create full prompt

  full_prompt = ""
  full_prompt += prompt_example_prefix
  full_prompt += "\nQuestion:\n"
  full_prompt += prompt.strip() + "\n"
  full_prompt = full_prompt.rstrip() + "\n" + prompt_no_explain_suffix
  full_prompt += "\nAnswer:\n"

  # Print system message and prompt

  out.print("")
  out.print("System Message")
  out.print("-"*74)
  out.print(system_msg)

  out.print("Prompt")
  out.print("-"*74)
  out.print(full_prompt.rstrip())

  # Create LLM messages

  msgs = [ SystemMessage(system_msg), HumanMessage(full_prompt) ]

  # Query the LLM

  if model in openai_chat_models:
    llm = ChatOpenAI(
      model        = model,
      temperature  = temperature,
      top_p        = top_p,
      max_tokens   = max_tokens,
    )
  elif model in nim_chat_models:
    llm = ChatNVIDIA(
      model       = model,
      temperature = temperature,
      top_p       = top_p,
      max_tokens  = max_tokens,
    )
  elif model in anyscale_chat_models:
    optional_params = {
      "top_p" : top_p,
    }
    llm = ChatAnyscale(
      model       = model,
      temperature = temperature,
      max_tokens  = max_tokens,
      model_kwargs = optional_params,
    )
  else:
    # should never reach here
    return

  for _ in range(10):
    try:
      with get_openai_callback() as cb:
        resp = llm.invoke(msgs)
        break
    except Exception as e:
      print("")
      print("ERROR: LLM query failed, retrying in 20 seconds")
      print(f"{type(e)}")
      print(f"{e}")
      print("")
      time.sleep(20)

  # Display the response

  out.print("")
  out.print("Reponse")
  out.print("-"*74)

  out.print("")
  print(resp.content)
  out.print("")

  # Display statistics

  out.print("Statistics")
  out.print("-"*74)

  out.print("")
  out.print(f"prompt_tokens = {cb.prompt_tokens}")
  out.print(f"resp_tokens   = {cb.completion_tokens}")
  out.print(f"total_tokens  = {cb.total_tokens}")
  out.print(f"total_cost    = {cb.total_cost}")
  out.print("")

  # Extract code from response

  if opts.output != "NONE":

    file = open( opts.output, 'w' )

    print( "", file=file )

    # Scan response for code using <CODE></CODE>

    found_code_lines = []
    found_code_start = False
    found_code_end   = False

    for line in iter(resp.content.splitlines()):

      if not found_code_start:
        if line.strip() == "<CODE>":
          found_code_start = True
        elif line.lstrip().startswith("<CODE>"):
          found_code_lines.append( line.lstrip().replace("<CODE>","") )
          found_code_start = True

      elif found_code_start and not found_code_end:
        if line.strip() == "</CODE>":
          found_code_end = True
        elif line.rstrip().endswith("</CODE>"):
          found_code_lines.append( line.rstrip().replace("</CODE>","") )
          found_code_end = True
        else:
          found_code_lines.append( line )

    if found_code_start and found_code_end:
      for line in found_code_lines:
        print( line, file=file )

    # If did not find code by looking for <CODE></CODE>, then scan response
    # for code using backticks

    if not found_code_start and not found_code_end:

      found_code_lines = []
      found_code_start = False
      found_code_end   = False

      for line in iter(resp.content.splitlines()):

        if not found_code_start:
          if line.lstrip().startswith("```"):
            found_code_start = True

        elif found_code_start and not found_code_end:
          if line.rstrip().endswith("```"):
            found_code_end = True
          else:
            found_code_lines.append( line )

      if found_code_start and found_code_end:
        for line in found_code_lines:
          print(line, file=file )

        # Print comment so we can track responses that did not use
        # <CODE></CODE> correctly

        print( "", file=file )
        if opts.lang == "verilog":
          comment_delim = "//"
        else:
          comment_delim = "#"
        print( comment_delim + " PYHDL-EVAL: response did not use <CODE></CODE> correctly", file=file )

    print( "", file=file )

    file.close()

main()
