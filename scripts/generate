#!/usr/bin/env python
#=========================================================================
# generate [options] prompt-filename
#=========================================================================
#
#  -v --verbose        Display the prompt
#  -h --help           Display this message
#  -l --list-models    List supported models
#     --model          LLM model to use (default: gpt-3.5-turbo)
#     --temperature    LLM model temperature (default: 0.85)
#     --top-p          LLM model top_p (default: 0.95)
#     --max-tokens     LLM model max_tokens (default: 1024)
#  -e --explain        Let the model include an explanation
#  -x --examples       Include the in-context examples
#  -r --rules          Include the in-context rules
#     --lang           What language to generate (default: verilog)
#     --output         File to write extracted code
#
# Use GPT to generate verilog or PyMTL from the given prompt.
#
# Author : Christopher Batten
# Date   : Mar 15, 2024
#

import argparse
import sys
import os
import re
import time

from adlrchat.langchain import ADLRChat
from adlrchat.langchain import LLMGatewayChat
from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain.schema   import SystemMessage, HumanMessage
from langchain.callbacks import get_openai_callback

#-------------------------------------------------------------------------
# Command line processing
#-------------------------------------------------------------------------

class ArgumentParserWithCustomError(argparse.ArgumentParser):
  def error( self, msg = "" ):
    if ( msg ): print("\n ERROR: %s" % msg)
    print("")
    file = open( sys.argv[0] )
    for ( lineno, line ) in enumerate( file ):
      if ( line[0] != '#' ): sys.exit(msg != "")
      if ( (lineno == 2) or (lineno >= 4) ): print( line[1:].rstrip("\n") )

def parse_cmdline():
  p = ArgumentParserWithCustomError( add_help=False )

  p.add_argument( "-h", "--help",        action="store_true" )
  p.add_argument( "-v", "--verbose",     action="store_true" )
  p.add_argument( "-l", "--list-models", action="store_true" )
  p.add_argument(       "--model",       type=str,   default="gpt-3.5-turbo" )
  p.add_argument(       "--temperature", type=float, default=0.8 )
  p.add_argument(       "--max-tokens",  type=int,   default=1024 )
  p.add_argument(       "--top-p",       type=float, default=0.95 )
  p.add_argument( "-e", "--explain",     action="store_true" )
  p.add_argument( "-x", "--examples",    action="store_true" )
  p.add_argument( "-r", "--rules",       action="store_true" )
  p.add_argument(       "--lang",        type=str,   default="verilog" )
  p.add_argument(       "--output",      type=str,   default="-" )
  p.add_argument( "prompt_filename" )

  opts = p.parse_args()
  if opts.help: p.error()
  return opts

#-------------------------------------------------------------------------
# Models
#-------------------------------------------------------------------------

adlr_chat_models = [
  "chipllama_70b_steer_delta",
  "mixtral-chat",
]

llm_gateway_chat_models = [
  "gpt-3.5-turbo",
  "gpt-4",
  "gpt-4-turbo",
]

nim_chat_models = [
  "ai-llama2-70b",
  "ai-llama3-70b",
  "ai-codellama-70b",
  "ai-gemma-7b",
  "ai-codegemma-7b",
  "ai-mistral-7b-instruct-v2",
  "ai-mixtral-8x7b-instruct",
]

#-------------------------------------------------------------------------
# VerboseOutput
#-------------------------------------------------------------------------

class VerboseOutput:

  def __init__( self, verbose ):
    self.verbose = verbose

  def print( self, string ):
    if self.verbose:
      print( string )

#-------------------------------------------------------------------------
# Context
#-------------------------------------------------------------------------

verilog_system_msg="""
You are a Verilog RTL designer that only writes code using correct
Verilog syntax.
"""

verilog_prompt_example_prefix_filename=os.path.dirname(__file__) + "/verilog-example-prefix.txt"
with open(verilog_prompt_example_prefix_filename) as f:
  verilog_prompt_example_prefix = f.read()

verilog_prompt_rules_suffix="""
Here are some additional rules and coding conventions.

 - declare all ports and signals as logic; do not to use wire or reg

 - for combinational logic with an always block do not explicitly specify
   the sensitivity list; instead use always @(*)

 - all sized numeric constants must have a size greater than zero
   (e.g, 0'b0 is not a valid expression)

 - an always block must read at least one signal otherwise it will never
   be executed; use an assign statement instead of an always block in
   situations where there is no need to read any signals

 - if the module uses a synchronous reset signal, this means the reset
   signal is sampled with respect to the clock. When implementing a
   synchronous reset signal, do not include posedge reset in the
   sensitivity list of any sequential always block.
"""

pymtl_system_msg="""
You are a PyMTL3 RTL designer that only writes code using correct
PyMTL syntax.
"""

pymtl_prompt_example_prefix_filename=os.path.dirname(__file__) + "/pymtl-example-prefix.txt"
with open(pymtl_prompt_example_prefix_filename) as f:
  pymtl_prompt_example_prefix = f.read()

pymtl_prompt_rules_suffix=""

pymtl_prompt_rules_suffix="""
Here are some additional rules and coding conventions.

 - The only Python decorators you can use are @update and @update_ff. Do
   not use any other kind of Python decorator in your solution.
"""

prompt_no_explain_suffix="""
Enclose your code with <CODE> and </CODE>. Only output the code snippet
and do NOT output anything else.
"""

#-------------------------------------------------------------------------
# Main
#-------------------------------------------------------------------------

def main():

  opts = parse_cmdline()

  # Check models

  if opts.list_models:
    print( "" )
    print( "ADLRChat Models" )

    for model in adlr_chat_models:
      print( f" - {model}" )

    print( "" )
    print( "LLMGatewayChat Models" )

    for model in llm_gateway_chat_models:
      print( f" - {model}" )

    print( "" )
    print( "NIM Models" )

    for model in nim_chat_models:
      print( f" - {model}" )

    print( "" )
    return

  if opts.model not in adlr_chat_models + llm_gateway_chat_models + nim_chat_models:
    print("")
    print(f"ERROR: Unknown model {opts.model}")
    print("")
    return

  # Check output language

  system_msg            = ""
  prompt_example_prefix = ""
  prompt_rules_suffix   = ""

  if opts.lang == "verilog":
    system_msg            = verilog_system_msg
    prompt_example_prefix = verilog_prompt_example_prefix
    prompt_rules_suffix   = verilog_prompt_rules_suffix
  elif opts.lang == "pymtl":
    system_msg            = pymtl_system_msg
    prompt_example_prefix = pymtl_prompt_example_prefix
    prompt_rules_suffix   = pymtl_prompt_rules_suffix

  # Check for an output file

  out = VerboseOutput( opts.verbose )

  # Log parameters

  problem = "?"
  if opts.prompt_filename.endswith("_prompt.txt"):
    problem = os.path.basename(opts.prompt_filename[:-11])

  model       = opts.model
  temperature = opts.temperature
  top_p       = opts.top_p
  max_tokens  = opts.max_tokens

  out.print( "" )
  out.print( f"problem     = {problem}"     )
  out.print( f"model       = {model}"       )
  out.print( f"temperature = {temperature}" )
  out.print( f"top_p       = {top_p}"       )
  out.print( f"max_tokens  = {max_tokens}"  )

  # Read the prompt file

  with open(opts.prompt_filename) as file:
    prompt = file.read()

  # Create full prompt

  full_prompt = ""
  if opts.examples:
    full_prompt += prompt_example_prefix

  full_prompt += "\nQuestion:\n"
  full_prompt += prompt.strip() + "\n"

  if opts.rules:
    full_prompt += prompt_rules_suffix

  if not opts.explain:
    full_prompt = full_prompt.rstrip() + "\n" + prompt_no_explain_suffix

  full_prompt += "\nAnswer:\n"

  # Print system message and prompt

  out.print("")
  out.print("System Message")
  out.print("-"*74)
  out.print(system_msg)

  out.print("Prompt")
  out.print("-"*74)
  out.print(full_prompt.rstrip())

  # Create LLM messages

  msgs = [ SystemMessage(system_msg), HumanMessage(full_prompt) ]

  # Query the LLM

  if model in adlr_chat_models:
    llm = ADLRChat(
      model       = model,
      temperature = temperature,
      top_p       = top_p,
      max_tokens  = max_tokens,
    )
  elif model in llm_gateway_chat_models:
    llm = LLMGatewayChat(
      model       = model,
      temperature = temperature,
      top_p       = top_p,
      max_tokens  = max_tokens,
    )
  elif model in nim_chat_models:
    llm = ChatNVIDIA(
      model       = model,
      temperature = temperature,
      top_p       = top_p,
      max_tokens  = max_tokens,
    )
  else:
    # should never reach here
    return

  for _ in range(10):
    try:
      with get_openai_callback() as cb:
        resp = llm.invoke(msgs)
        break
    except Exception as e:
      print("")
      print("ERROR: LLM query failed, retrying in 20 seconds")
      print(f"{type(e)}")
      print(f"{e}")
      print("")
      time.sleep(20)

  # Display the response

  out.print("")
  out.print("Reponse")
  out.print("-"*74)

  out.print("")
  print(resp.content)
  out.print("")

  # Display statistics

  out.print("Statistics")
  out.print("-"*74)

  out.print("")
  out.print(f"prompt_tokens = {cb.prompt_tokens}")
  out.print(f"resp_tokens   = {cb.completion_tokens}")
  out.print(f"total_tokens  = {cb.total_tokens}")
  out.print(f"total_cost    = {cb.total_cost}")
  out.print("")

  # Extract code from response

  file = open( opts.output, 'w' )

  print( "", file=file )

  # Scan response for code using <CODE></CODE>

  found_code_lines = []
  found_code_start = False
  found_code_end   = False

  for line in iter(resp.content.splitlines()):

    if not found_code_start:
      if line.strip() == "<CODE>":
        found_code_start = True
      elif line.lstrip().startswith("<CODE>"):
        found_code_lines.append( line.lstrip().replace("<CODE>","") )
        found_code_start = True

    elif found_code_start and not found_code_end:
      if line.strip() == "</CODE>":
        found_code_end = True
      elif line.rstrip().endswith("</CODE>"):
        found_code_lines.append( line.rstrip().replace("</CODE>","") )
        found_code_end = True
      else:
        found_code_lines.append( line )

  if found_code_start and found_code_end:
    for line in found_code_lines:
      print( line, file=file )

  # If did not find code by looking for <CODE></CODE>, then scan response
  # for code using backticks

  if not found_code_start and not found_code_end:

    found_code_lines = []
    found_code_start = False
    found_code_end   = False

    for line in iter(resp.content.splitlines()):

      if not found_code_start:
        if line.lstrip().startswith("```"):
          found_code_start = True

      elif found_code_start and not found_code_end:
        if line.rstrip().endswith("```"):
          found_code_end = True
        else:
          found_code_lines.append( line )

    if found_code_start and found_code_end:
      for line in found_code_lines:
        print(line, file=file )

      # Print comment so we can track responses that did not use
      # <CODE></CODE> correctly

      print( "", file=file )
      if opts.lang == "verilog":
        comment_delim = "//"
      elif opts.lang == "pymtl":
        comment_delim = "#"
      print( comment_delim + " PYHDL-EVAL: response did not use <CODE></CODE> correctly", file=file )

  print( "", file=file )

  file.close()

main()
