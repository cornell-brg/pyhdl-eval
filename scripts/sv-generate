#!/usr/bin/env python
#=========================================================================
# sv-verilog [options] prompt-filename
#=========================================================================
#
#  -v --verbose        Display the prompt
#  -h --help           Display this message
#  -l --list-models    List supported models
#  -m --model          LLM model to use (default: gpt-3.5-turbo)
#  -t --temperature    LLM model temperature (default: 0.85)
#  -p --top-p          LLM model top_p (default: 0.95)
#  -n --max-tokens     LLM model max_tokens (default: 1024)
#  -e --explain        Let the model include an explanation
#  -o --output         File to write verilog to (default: stdout)
#
# Use GPT to generate verilog from the given prompt.
#
# Author : Christopher Batten
# Date   : Feb 22, 2024
#

import argparse
import sys
import os
import re
import time

from adlrchat.langchain import ADLRChat
from adlrchat.langchain import LLMGatewayChat
from langchain.schema   import SystemMessage, HumanMessage
from langchain.callbacks import get_openai_callback

#-------------------------------------------------------------------------
# Command line processing
#-------------------------------------------------------------------------

class ArgumentParserWithCustomError(argparse.ArgumentParser):
  def error( self, msg = "" ):
    if ( msg ): print("\n ERROR: %s" % msg)
    print("")
    file = open( sys.argv[0] )
    for ( lineno, line ) in enumerate( file ):
      if ( line[0] != '#' ): sys.exit(msg != "")
      if ( (lineno == 2) or (lineno >= 4) ): print( line[1:].rstrip("\n") )

def parse_cmdline():
  p = ArgumentParserWithCustomError( add_help=False )

  p.add_argument( "-h", "--help",        action="store_true" )
  p.add_argument( "-v", "--verbose",     action="store_true" )
  p.add_argument( "-l", "--list-models", action="store_true" )
  p.add_argument( "-m", "--model",       type=str,   default="gpt-3.5-turbo" )
  p.add_argument( "-t", "--temperature", type=float, default=0.8 )
  p.add_argument( "-n", "--max-tokens",  type=int,   default=1024 )
  p.add_argument( "-p", "--top-p",       type=float, default=0.95 )
  p.add_argument( "-e", "--explain",     action="store_true" )
  p.add_argument( "-o", "--output",      type=str, default="-" )
  p.add_argument( "prompt_filename" )

  opts = p.parse_args()
  if opts.help: p.error()
  return opts

#-------------------------------------------------------------------------
# Models
#-------------------------------------------------------------------------

adlr_chat_models = [
  "chipllama_70b_steer_delta",
  "mixtral-chat",
]

llm_gateway_chat_models = [
  "gpt-3.5-turbo",
  "gpt-4",
]

#-------------------------------------------------------------------------
# VerboseOutput
#-------------------------------------------------------------------------

class VerboseOutput:

  def __init__( self, verbose ):
    self.verbose = verbose

  def print( self, string ):
    if self.verbose:
      print( string )

#-------------------------------------------------------------------------
# Context
#-------------------------------------------------------------------------

system_msg="""
You are a Verilog RTL designer that only writes code using correct
Verilog syntax.
"""

prompt_rules_suffix="""
Here are some additional rules and coding conventions.

 - declare all ports and signals as logic; do not to use wire or reg

 - for combinational logic with an always block do not explicitly specify
   the sensitivity list; instead use always @(*)

 - all sized numeric constants must have a size greater than zero
   (e.g, 0'b0 is not a valid expression)

 - an always block must read at least one signal otherwise it will never
   be executed; use an assign statement instead of an always block in
   situations where there is no need to read any signals
"""

prompt_no_explain_suffix="""
Please do not include any explanations in your response.
"""

#-------------------------------------------------------------------------
# Main
#-------------------------------------------------------------------------

def main():

  opts = parse_cmdline()

  # Check models

  if opts.list_models:
    print( "" )
    print( "ADLRChat Models" )

    for model in adlr_chat_models:
      print( f" - {model}" )

    print( "" )
    print( "LLMGatewayChat Models" )

    for model in llm_gateway_chat_models:
      print( f" - {model}" )

    print( "" )
    return

  if opts.model not in adlr_chat_models + llm_gateway_chat_models:
    print("")
    print(f"ERROR: Unknown model {opts.model}")
    print("")
    return

  # Check for an output file

  out = VerboseOutput( opts.verbose )

  # Log parameters

  problem = "?"
  if opts.prompt_filename.endswith("_prompt.txt"):
    problem = os.path.basename(opts.prompt_filename[:-11])

  model       = opts.model
  temperature = opts.temperature
  top_p       = opts.top_p
  max_tokens  = opts.max_tokens

  out.print( "" )
  out.print( f"problem     = {problem}"     )
  out.print( f"model       = {model}"       )
  out.print( f"temperature = {temperature}" )
  out.print( f"top_p       = {top_p}"       )
  out.print( f"max_tokens  = {max_tokens}"  )

  # Read the prompt file

  with open(opts.prompt_filename) as file:
    prompt = file.read()

  # Add prompt suffix

  prompt = prompt.rstrip() + "\n" + prompt_rules_suffix

  if not opts.explain:
    prompt = prompt.rstrip() + "\n" + prompt_no_explain_suffix

  # Print system message and prompt

  out.print("")
  out.print("System Message")
  out.print("-"*74)
  out.print(system_msg)

  out.print("Prompt")
  out.print("-"*74)
  out.print(prompt.rstrip())

  # Create LLM messages

  msgs = [ SystemMessage(system_msg), HumanMessage(prompt) ]

  # Query the LLM

  if model in adlr_chat_models:
    llm = ADLRChat(
      model       = model,
      temperature = temperature,
      top_p       = top_p,
      max_tokens  = max_tokens,
    )
  elif model in llm_gateway_chat_models:
    llm = LLMGatewayChat(
      model       = model,
      temperature = temperature,
      top_p       = top_p,
      max_tokens  = max_tokens,
    )
  else:
    # should never reach here
    return

  for _ in range(10):
    try:
      with get_openai_callback() as cb:
        resp = llm.invoke(msgs)
        break
    except Exception as e:
      print("")
      print("ERROR: LLM query failed, retrying in 20 seconds")
      print(f"{type(e)}")
      print(f"{e}")
      print("")
      time.sleep(20)

  # Display the response

  out.print("")
  out.print("Reponse")
  out.print("-"*74)

  out.print("")
  print(resp.content)
  out.print("")

  # Display statistics

  out.print("Statistics")
  out.print("-"*74)

  out.print("")
  out.print(f"prompt_tokens = {cb.prompt_tokens}")
  out.print(f"resp_tokens   = {cb.completion_tokens}")
  out.print(f"total_tokens  = {cb.total_tokens}")
  out.print(f"total_cost    = {cb.total_cost}")
  out.print("")

  # Write to output file (scan for content between triple back ticks or
  # scan for content between module/endmodule)

  if opts.output != '-':
    with open( opts.output, 'w' ) as file:

      print( "", file=file )

      found_first_backticks  = False
      found_second_backticks = False
      found_module           = False
      found_endmodule        = False

      for line in iter(resp.content.splitlines()):

        if not found_first_backticks and not found_module:
          if line.startswith("```"):
            found_first_backticks = True
          if line.startswith("module TopModule"):
            found_module = True
            print( line, file=file )

        elif found_first_backticks and not found_second_backticks:
          if line.startswith("```"):
            found_second_backticks = True
          else:
            print( line, file=file )

        elif found_module and not found_endmodule:
          print( line, file=file )
          if line.startswith("endmodule"):
            found_endmodule = True

      print( "", file=file )

main()
